{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aad5fb7",
   "metadata": {},
   "source": [
    "# Bengali BPE Tokenizer Training Notebook\n",
    "\n",
    "Steps:\n",
    "1. Download Bengali Wikipedia dump\n",
    "2. Extract with WikiExtractor\n",
    "3. Normalize & clean\n",
    "4. Train custom BPE (educational)\n",
    "5. Train Hugging Face Tokenizers BPE\n",
    "6. Evaluate compression metrics\n",
    "7. Save & push to Hugging Face Hub\n",
    "8. Prepare artifacts for Hugging Face Space\n",
    "\n",
    "Set `HF_TOKEN` as an environment variable for upload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf98cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories ready.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, json, math, shutil, tarfile, bz2, re\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "EXTRACT_DIR = DATA_DIR / 'extracted'\n",
    "CLEAN_TEXT = DATA_DIR / 'clean_corpus.txt'\n",
    "MODEL_DIR = BASE_DIR / 'model'\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "for d in [DATA_DIR, RAW_DIR, EXTRACT_DIR]:\n",
    "    d.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print('Directories ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfad08",
   "metadata": {},
   "source": [
    "## 1. Download Bengali Wikipedia Dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a005876",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_url = 'https://dumps.wikimedia.org/bnwiki/latest/bnwiki-latest-pages-articles.xml.bz2'\n",
    "dump_path = RAW_DIR / 'bnwiki-latest-pages-articles.xml.bz2'\n",
    "if not dump_path.exists():\n",
    "    resp = requests.get(dump_url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    with open(dump_path, 'wb') as f, tqdm(total=total, unit='B', unit_scale=True, desc='Downloading dump') as pbar:\n",
    "        for chunk in resp.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "else:\n",
    "    print('Dump already downloaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316f57a",
   "metadata": {},
   "source": [
    "## 2. Extract with WikiExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02973731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2, re, json, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "dump_path = dump_path\n",
    "extract_output_dir = EXTRACT_DIR / \"bnwiki_stream\"\n",
    "if extract_output_dir.exists():\n",
    "    shutil.rmtree(extract_output_dir)\n",
    "extract_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "title_re   = re.compile(r\"<title>(.*?)</title>\")\n",
    "text_re    = re.compile(r\"<text[^>]*>(.*?)</text>\", re.DOTALL)\n",
    "redirect_re= re.compile(r\"<redirect\")\n",
    "\n",
    "def iter_pages(bz2_file):\n",
    "    buf = []\n",
    "    inside = False\n",
    "    with bz2.open(bz2_file, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if \"<page>\" in line:\n",
    "                inside = True\n",
    "                buf = [line]\n",
    "            elif \"</page>\" in line and inside:\n",
    "                buf.append(line)\n",
    "                yield \"\".join(buf)\n",
    "                inside = False\n",
    "            elif inside:\n",
    "                buf.append(line)\n",
    "\n",
    "def clean_wikitext(txt):\n",
    "    txt = re.sub(r\"\\{\\{.*?\\}\\}\", \" \", txt, flags=re.DOTALL)\n",
    "    txt = re.sub(r\"\\[\\[File:.*?\\]\\]\", \" \", txt, flags=re.IGNORECASE | re.DOTALL)\n",
    "    txt = re.sub(r\"\\[\\[(?:[^|\\]]*\\|)?([^|\\]]+)\\]\\]\", r\"\\1\", txt)\n",
    "    txt = re.sub(r\"<ref.*?</ref>\", \" \", txt, flags=re.DOTALL)\n",
    "    txt = re.sub(r\"<.*?>\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "batch = []\n",
    "batch_size = 4000\n",
    "file_index = 0\n",
    "page_count = 0\n",
    "\n",
    "for page_xml in iter_pages(dump_path):\n",
    "    if redirect_re.search(page_xml):\n",
    "        continue\n",
    "    tm = title_re.search(page_xml)\n",
    "    xm = text_re.search(page_xml)\n",
    "    if not (tm and xm):\n",
    "        continue\n",
    "    title = tm.group(1)\n",
    "    body = clean_wikitext(xm.group(1))\n",
    "    if not body:\n",
    "        continue\n",
    "    batch.append({\"title\": title, \"text\": body})\n",
    "    page_count += 1\n",
    "    if len(batch) >= batch_size:\n",
    "        out_file = extract_output_dir / f\"wiki_{file_index:05d}.json\"\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\") as out:\n",
    "            for obj in batch:\n",
    "                out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        batch.clear()\n",
    "        file_index += 1\n",
    "\n",
    "if batch:\n",
    "    out_file = extract_output_dir / f\"wiki_{file_index:05d}.json\"\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        for obj in batch:\n",
    "            out.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Streaming extraction complete. Pages: {page_count}, Files: {file_index + 1}\")\n",
    "print(\"Sample file:\", out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830aa69",
   "metadata": {},
   "source": [
    "## 3. Normalize & Clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, unicodedata, gzip\n",
    "from src.utils import normalize_line\n",
    "\n",
    "SOURCE_JSON_DIR = EXTRACT_DIR / \"bnwiki_stream\"\n",
    "print(\"Using source JSON directory:\", SOURCE_JSON_DIR)\n",
    "\n",
    "import json\n",
    "from src.utils import normalize_line\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN_TEXT = DATA_DIR / \"clean_corpus.txt\"\n",
    "if CLEAN_TEXT.exists():\n",
    "    CLEAN_TEXT.unlink()\n",
    "\n",
    "lines_written = 0\n",
    "for jsf in SOURCE_JSON_DIR.rglob(\"*.json\"):\n",
    "    with open(jsf, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            text = obj.get(\"text\", \"\")\n",
    "            if not text:\n",
    "                continue\n",
    "            for raw_line in text.split(\"\\n\"):\n",
    "                norm = normalize_line(raw_line)\n",
    "                if norm:\n",
    "                    lines_written += 1\n",
    "                    with open(CLEAN_TEXT, \"a\", encoding=\"utf-8\") as out:\n",
    "                        out.write(norm + \"\\n\")\n",
    "\n",
    "print(\"Clean corpus lines:\", lines_written)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2b885",
   "metadata": {},
   "source": [
    "## 4. Train Custom BPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6212d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CustomBPE] Merge 500: ('গ', 'ু') | current vocab est=505\n",
      "[CustomBPE] Merge 1000: ('ার', 'ে</w>') | current vocab est=1005\n",
      "[CustomBPE] Merge 1000: ('ার', 'ে</w>') | current vocab est=1005\n",
      "[CustomBPE] Merge 1500: ('মহ', 'িল') | current vocab est=1505\n",
      "[CustomBPE] Merge 1500: ('মহ', 'িল') | current vocab est=1505\n",
      "[CustomBPE] Merge 2000: ('ম', 'য়') | current vocab est=2005\n",
      "[CustomBPE] Merge 2000: ('ম', 'য়') | current vocab est=2005\n",
      "[CustomBPE] Merge 2500: ('ুয়', 'ারি') | current vocab est=2505\n",
      "[CustomBPE] Merge 2500: ('ুয়', 'ারি') | current vocab est=2505\n",
      "[CustomBPE] Merge 3000: ('থ', 'া</w>') | current vocab est=3005\n",
      "[CustomBPE] Merge 3000: ('থ', 'া</w>') | current vocab est=3005\n",
      "[CustomBPE] Merge 3500: ('উ', 'দ্ধ') | current vocab est=3505\n",
      "[CustomBPE] Merge 3500: ('উ', 'দ্ধ') | current vocab est=3505\n",
      "[CustomBPE] Merge 4000: ('ব', 'ক্') | current vocab est=4005\n",
      "[CustomBPE] Merge 4000: ('ব', 'ক্') | current vocab est=4005\n",
      "[CustomBPE] Merge 4500: ('প্র', 'কৃতি</w>') | current vocab est=4505\n",
      "[CustomBPE] Merge 4500: ('প্র', 'কৃতি</w>') | current vocab est=4505\n",
      "[CustomBPE] Merge 5000: ('সে', 'খান') | current vocab est=5005\n",
      "[CustomBPE] Merge 5000: ('সে', 'খান') | current vocab est=5005\n",
      "[CustomBPE] Merge 5500: ('8', ',</w>') | current vocab est=5505\n",
      "[CustomBPE] Merge 5500: ('8', ',</w>') | current vocab est=5505\n",
      "[CustomBPE] Merge 6000: ('ইউনি', 'ট</w>') | current vocab est=6005\n",
      "[CustomBPE] Merge 6000: ('ইউনি', 'ট</w>') | current vocab est=6005\n",
      "[CustomBPE] Merge 6500: ('তু', 'ল</w>') | current vocab est=6505\n",
      "[CustomBPE] Merge 6500: ('তু', 'ল</w>') | current vocab est=6505\n",
      "[CustomBPE] Merge 7000: ('অ', '-') | current vocab est=7005\n",
      "[CustomBPE] Merge 7000: ('অ', '-') | current vocab est=7005\n",
      "[CustomBPE] Merge 7500: ('করি', 'ম</w>') | current vocab est=7505\n",
      "[CustomBPE] Merge 7500: ('করি', 'ম</w>') | current vocab est=7505\n",
      "[CustomBPE] Merge 8000: ('উপজেলা', \"'''</w>\") | current vocab est=8005\n",
      "[CustomBPE] Merge 8000: ('উপজেলা', \"'''</w>\") | current vocab est=8005\n",
      "[CustomBPE] Merge 8500: ('স্টেড', 'িয়াম') | current vocab est=8505\n",
      "[CustomBPE] Merge 8500: ('স্টেড', 'িয়াম') | current vocab est=8505\n",
      "[CustomBPE] Merge 9000: ('ে', 'ন্স</w>') | current vocab est=9005\n",
      "[CustomBPE] Merge 9000: ('ে', 'ন্স</w>') | current vocab est=9005\n",
      "[CustomBPE] Merge 9500: ('অকাদে', 'মি</w>') | current vocab est=9505\n",
      "[CustomBPE] Merge 9500: ('অকাদে', 'মি</w>') | current vocab est=9505\n",
      "[CustomBPE] Merge 10000: ('ইউ', 'এস</w>') | current vocab est=10005\n",
      "[CustomBPE] Merge 10000: ('ইউ', 'এস</w>') | current vocab est=10005\n",
      "[CustomBPE] Merge 10500: ('দৈ', 'নন্দ') | current vocab est=10505\n",
      "[CustomBPE] Merge 10500: ('দৈ', 'নন্দ') | current vocab est=10505\n",
      "[CustomBPE] Merge 11000: ('হা', 'ফি') | current vocab est=11005\n",
      "[CustomBPE] Merge 11000: ('হা', 'ফি') | current vocab est=11005\n",
      "[CustomBPE] Merge 11500: ('সু', 'ভা') | current vocab est=11505\n",
      "[CustomBPE] Merge 11500: ('সু', 'ভা') | current vocab est=11505\n",
      "[CustomBPE] Merge 12000: ('মত', 'ই</w>') | current vocab est=12005\n",
      "[CustomBPE] Merge 12000: ('মত', 'ই</w>') | current vocab est=12005\n",
      "[CustomBPE] Merge 12500: ('চ', 'ারি') | current vocab est=12505\n",
      "[CustomBPE] Merge 12500: ('চ', 'ারি') | current vocab est=12505\n",
      "[CustomBPE] Merge 13000: ('গো', 'য়াল') | current vocab est=13005\n",
      "[CustomBPE] Merge 13000: ('গো', 'য়াল') | current vocab est=13005\n",
      "[CustomBPE] Merge 13500: ('চৌধুর', 'ী') | current vocab est=13505\n",
      "[CustomBPE] Merge 13500: ('চৌধুর', 'ী') | current vocab est=13505\n",
      "[CustomBPE] Merge 14000: ('রম', 'জান</w>') | current vocab est=14005\n",
      "[CustomBPE] Merge 14000: ('রম', 'জান</w>') | current vocab est=14005\n",
      "[CustomBPE] Merge 14500: ('কর্ম', 'সংস্থান</w>') | current vocab est=14505\n",
      "[CustomBPE] Merge 14500: ('কর্ম', 'সংস্থান</w>') | current vocab est=14505\n",
      "[CustomBPE] Merge 15000: ('গোলার্', 'ধের</w>') | current vocab est=15005\n",
      "[CustomBPE] Merge 15000: ('গোলার্', 'ধের</w>') | current vocab est=15005\n",
      "[CustomBPE] Merge 15500: ('191', '2)</w>') | current vocab est=15505\n",
      "[CustomBPE] Merge 15500: ('191', '2)</w>') | current vocab est=15505\n",
      "Custom BPE merges: 15995 | vocab size (approx tokens seen): 15846\n",
      "Custom BPE merges: 15995 | vocab size (approx tokens seen): 15846\n"
     ]
    }
   ],
   "source": [
    "from src.custom_bpe import CustomBPE, normalize_bengali\n",
    "\n",
    "TARGET_VOCAB = 16000  # > 5000 as per assignment\n",
    "MIN_FREQ = 3\n",
    "CLEAN_TEXT_LINE_LIMIT = 5000  # set to int to cap lines; None uses the full clean corpus\n",
    "\n",
    "custom_bpe = CustomBPE(\n",
    "    vocab_size=TARGET_VOCAB,\n",
    "    min_freq=MIN_FREQ,\n",
    "    progress_every=500,\n",
    "    normalize_fn=lambda s: normalize_bengali(s, map_digits=True, keep_latin=True),\n",
    "    dropout=0.05,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "def line_iter(limit=None):\n",
    "    effective_limit = CLEAN_TEXT_LINE_LIMIT if limit is None else limit\n",
    "    with open(CLEAN_TEXT, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if effective_limit is not None and i >= effective_limit:\n",
    "                break\n",
    "            yield line\n",
    "\n",
    "custom_result = custom_bpe.train(line_iter())\n",
    "print(f\"Custom BPE merges: {len(custom_result.merges)} | vocab size (approx tokens seen): {len(custom_result.vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ee130",
   "metadata": {},
   "source": [
    "## 5. Train Hugging Face Tokenizers BPE (Optional, For Comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e65d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, processors\n",
    "\n",
    "hf_tokenizer_path = MODEL_DIR / 'hf_tokenizer.json'\n",
    "if not hf_tokenizer_path.exists():\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = normalizers.Sequence([\n",
    "        normalizers.NFC()\n",
    "    ])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=8000,\n",
    "        min_frequency=5,\n",
    "        special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
    "    )\n",
    "\n",
    "    def batch_iterator(batch_size=1000, limit=None):\n",
    "        effective_limit = CLEAN_TEXT_LINE_LIMIT if limit is None else limit\n",
    "        with open(CLEAN_TEXT, 'r', encoding='utf-8') as f:\n",
    "            batch = []\n",
    "            used = 0\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                batch.append(line)\n",
    "                used += 1\n",
    "                if len(batch) == batch_size:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "                if effective_limit is not None and used >= effective_limit:\n",
    "                    break\n",
    "            if batch:\n",
    "                yield batch\n",
    "\n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single='[CLS] $A [SEP]',\n",
    "        pair='[CLS] $A [SEP] $B:1 [SEP]:1',\n",
    "        special_tokens=[('[CLS]', tokenizer.token_to_id('[CLS]')), ('[SEP]', tokenizer.token_to_id('[SEP]'))]\n",
    "    )\n",
    "    tokenizer.save(str(hf_tokenizer_path))\n",
    "else:\n",
    "    tokenizer = Tokenizer.from_file(str(hf_tokenizer_path))\n",
    "print('HF tokenizer vocab size:', tokenizer.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5885cb",
   "metadata": {},
   "source": [
    "## 6. Evaluate Compression Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e786c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom BPE Metrics: {\n",
      "  \"lines_sampled\": 5000,\n",
      "  \"total_chars\": 26444005,\n",
      "  \"total_bytes\": 67426621,\n",
      "  \"total_tokens\": 5811945,\n",
      "  \"chars_per_token\": 4.549940682508179,\n",
      "  \"bytes_per_token\": 11.601386627024171,\n",
      "  \"assignment_compression_ratio\": 4.549940682508179,\n",
      "  \"approx_byte_compression_ratio\": 5.8006933135120855\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from src.utils import compute_basic_metrics\n",
    "\n",
    "metrics_sample_size = 5000\n",
    "if 'CLEAN_TEXT_LINE_LIMIT' in globals() and CLEAN_TEXT_LINE_LIMIT is not None:\n",
    "    metrics_sample_size = min(metrics_sample_size, CLEAN_TEXT_LINE_LIMIT)\n",
    "\n",
    "metrics_custom = compute_basic_metrics(CLEAN_TEXT, custom_bpe.encode, sample_size=metrics_sample_size)\n",
    "# metrics_hf = compute_basic_metrics(CLEAN_TEXT, lambda t: tokenizer.encode(t).ids, sample_size=metrics_sample_size)\n",
    "\n",
    "print('Custom BPE Metrics:', json.dumps(metrics_custom, indent=2, ensure_ascii=False))\n",
    "# print('HuggingFace BPE Metrics:', json.dumps(metrics_hf, indent=2, ensure_ascii=False))\n",
    "\n",
    "# assert metrics_custom['assignment_compression_ratio'] >= 3 or metrics_hf['assignment_compression_ratio'] >= 3, \\\n",
    "#     'Need ratio >= 3 for assignment. Consider increasing vocab or cleaning.'\n",
    "assert metrics_custom['assignment_compression_ratio'] >= 3, \\\n",
    "    'Need ratio >= 3 for assignment. Consider increasing vocab or cleaning.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df08663",
   "metadata": {},
   "source": [
    "## 7. Save Artifacts & (Optional) Push to Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0482266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tokenizer saved to: c:\\Personal Learning\\ERA V4\\Assignment 11\\tsai_assignment_11\\model\\custom_tokenizer.json\n",
      "Artifacts saved.\n"
     ]
    }
   ],
   "source": [
    "from src.utils import save_json\n",
    "\n",
    "ARTIFACTS_DIR = MODEL_DIR / 'artifacts'\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "save_json(metrics_custom, ARTIFACTS_DIR / 'metrics_custom.json')\n",
    "# save_json(metrics_hf, ARTIFACTS_DIR / 'metrics_hf.json')\n",
    "\n",
    "CUSTOM_TOKENIZER_PATH = MODEL_DIR / 'custom_tokenizer.json'\n",
    "custom_bpe.save(CUSTOM_TOKENIZER_PATH)\n",
    "\n",
    "# Save custom merges\n",
    "with open(ARTIFACTS_DIR / 'custom_merges.txt', 'w', encoding='utf-8') as f:\n",
    "    for a,b in custom_bpe.merges:\n",
    "        f.write(f'{a} {b}\\n')\n",
    "\n",
    "print('Custom tokenizer saved to:', CUSTOM_TOKENIZER_PATH)\n",
    "print('Artifacts saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4beb532",
   "metadata": {},
   "source": [
    "### Hugging Face Upload (Requires HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder, create_repo, upload_file\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "repo_name = 'bengali-bpe-tokenizer'\n",
    "user = None\n",
    "if hf_token:\n",
    "    api = HfApi(token=hf_token)\n",
    "    user = api.whoami()['name']\n",
    "    full_repo = f'{user}/{repo_name}'\n",
    "    try:\n",
    "        create_repo(full_repo, private=False)\n",
    "    except Exception as e:\n",
    "        print('Repo may already exist:', e)\n",
    "    # Upload custom tokenizer artifacts\n",
    "    upload_file(path_or_fileobj=str(CUSTOM_TOKENIZER_PATH), path_in_repo='custom_tokenizer.json', repo_id=full_repo)\n",
    "    upload_file(path_or_fileobj=str(ARTIFACTS_DIR / 'custom_merges.txt'), path_in_repo='custom_merges.txt', repo_id=full_repo)\n",
    "    upload_file(path_or_fileobj=str(ARTIFACTS_DIR / 'metrics_custom.json'), path_in_repo='metrics_custom.json', repo_id=full_repo)\n",
    "    print('Uploaded custom tokenizer to:', f'https://huggingface.co/{full_repo}')\n",
    "else:\n",
    "    print('HF_TOKEN not set; skipping upload.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1534dd",
   "metadata": {},
   "source": [
    "## 8. Prepare Space Files\n",
    "Place `custom_tokenizer.json` and `space/app.py` in a new Space repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_DIR = BASE_DIR / 'space'\n",
    "SPACE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "space_tokenizer_path = SPACE_DIR / 'custom_tokenizer.json'\n",
    "shutil.copy(CUSTOM_TOKENIZER_PATH, space_tokenizer_path)\n",
    "shutil.copy(BASE_DIR / 'src' / 'custom_bpe.py', SPACE_DIR / 'custom_bpe.py')\n",
    "print('Copied custom tokenizer and supporting code to space directory.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcc6b4",
   "metadata": {},
   "source": [
    "## 9. Quick Test Decode/Encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'বাংলা ভাষা একটি ইন্দো-আর্য ভাষা।'\n",
    "enc = tokenizer.encode(sample_text)\n",
    "print(enc.tokens)\n",
    "print('Decoded back:', tokenizer.decode(enc.ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00166e",
   "metadata": {},
   "source": [
    "## 10. Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9be047",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'custom_bpe_vocab_estimate': len(custom_bpe.token2id),\n",
    "    'hf_bpe_vocab_size': tokenizer.get_vocab_size(),\n",
    "    'custom_metrics': metrics_custom,\n",
    "    'hf_metrics': metrics_hf\n",
    "}\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4b845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsai_assignment_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
